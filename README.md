# ğŸ¤– AI Helper - Intelligent Assistant for Everything

A comprehensive AI-powered application built with **Streamlit**, combining web scraping, document analysis, research, and essay generation into a single modern interface.

![Python](https://img.shields.io/badge/python-v3.9+-blue.svg)  
![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)  
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-green.svg)  
![LangChain](https://img.shields.io/badge/LangChain-0.1+-purple.svg)

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Running the App](#-running-the-app)
- [Modules](#-modules)
- [Requirements](#-requirements)
- [Project Structure](#-project-structure)

## âœ¨ Features

### ğŸ•·ï¸ **Web DOM Scraper**
- Automatic content extraction from websites  
- AI-powered parsing for specific data  
- Support for static and dynamic pages  
- Chrome WebDriver integration  

### ğŸ“Š **File Data Analysis (RAG)**
- Intelligent analysis of PDF, DOCX, and TXT files  
- Retrieval-Augmented Generation technology  
- Document Q&A system  
- Vector search with OpenAI embeddings  

### ğŸ” **Web Research**
- Multi-agent research system  
- Automatic fact-checking  
- Bias detection and analysis  
- APA-style citations  
- 5-step research process  

### âœï¸ **Essay Writing**
- AI-generated essays (200â€“220 words)  
- Automatic review and improvement  
- Keyword generation for presentations  
- Email export functionality  

## ğŸš€ Installation

### Prerequisites
- Python 3.9+  
- Installed Chrome browser  
- OpenAI API key  
- Tavily API key (for research)  

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/ai-helper.git
cd ai-helper
```

### 2. Create a Virtual Environment
```bash
python -m venv ai_helper_env
source ai_helper_env/bin/activate  # Linux/Mac
# or
ai_helper_env\Scriptsctivate     # Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### 1. Create the .env File
```bash
cp .env.example .env
```

### 2. Set API Keys in .env
```env
# OpenAI API
OPENAI_API_KEY=your_openai_api_key_here

# Tavily API (for research)
TAVILY_API_KEY=your_tavily_api_key_here

# Email configuration
SENDER_EMAIL=your_email
GOOGLE_APP_PASSWORD=your_google_app_password_here
```

### 3. Get Your API Keys

#### OpenAI API:
1. Visit [OpenAI Platform](https://platform.openai.com/)
2. Create an account and go to API Keys
3. Generate a new API key

#### Tavily API:
1. Visit [Tavily](https://tavily.com/)
2. Register and get your API key

#### Google App Password:
1. Enable 2FA on your Google account  
2. Generate an App Password in account settings  
3. Use this password instead of your regular password  

## ğŸƒ Running the App

```bash
streamlit run main.py
```

The application will open in your browser at `http://localhost:8501`

## ğŸ§© Modules

### ğŸ“ Project Structure
```
ai-helper/
â”‚
â”œâ”€â”€ main.py                 # Main application
â”œâ”€â”€ essays.py               # Essay writing module
â”œâ”€â”€ research.py             # Web research module  
â”œâ”€â”€ RAG.py                  # Document analysis module
â”œâ”€â”€ web_scraper.py          # Web scraping module
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ .env.example            # Example environment file
â”œâ”€â”€ chromedriver.exe        # Chrome WebDriver
â””â”€â”€ README.md               # Documentation
```

### ğŸ”§ Main Components

#### main.py
- Streamlit application  
- Module navigation  
- Responsive design  

#### essays.py
- LangGraph agents for essay workflow  
- OpenAI GPT-4 integration  
- Email export functionality  

#### research.py
- Multi-agent research system  
- Tavily search integration  
- Fact-checking and bias detection  
- Citation formatting  

#### RAG.py
- Document loading (PDF, DOCX, TXT)  
- Text chunking and embeddings  
- Chroma vector database  
- RetrievalQA chains  

#### web_scraper.py
- Selenium WebDriver automation  
- BeautifulSoup HTML parsing  
- AI-powered content extraction  
- Smart content filtering  

## ğŸ“¦ Requirements

### Python Packages
```txt
streamlit
langchain
langchain-openai
langchain-community
langgraph
openai
tavily-python
selenium
beautifulsoup4
python-dotenv
chromadb
```

## ğŸš¨ Troubleshooting

### Common Issues

#### ChromeDriver Errors
```bash
# Make sure ChromeDriver is in PATH or in the project root folder
# Verify compatibility with your installed Chrome version
```

#### OpenAI API Errors
```bash
# Check if your API key is valid
# Verify available credits on your OpenAI account
# Check for rate limits
```

#### Memory Issues with Large Files
```bash
# Reduce chunk_size in RAG.py
# Use smaller documents
# Increase system memory
```

## ğŸ›¡ï¸ Security

### API Keys
- **Never** share API keys publicly  
- Use `.env` files for credentials  
- Rotate keys regularly  

### Data
- Local document processing  
- No persistent storage of sensitive data  
- GDPR-compliant processing  

## ğŸ“ˆ Future Improvements

- [ ] **HTML Frontend + Docker**  
- [ ] **API Endpoints**  

<p align="center">
  <strong>Made with â¤ï¸ for the AI community</strong>
</p>

<p align="center">
  <a href="#top">â¬†ï¸ Back to Top</a>
</p>
